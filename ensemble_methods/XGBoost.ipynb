{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eaa605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9a8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../cleaned_datasets/X_train.csv')\n",
    "X_test = pd.read_csv('../cleaned_datasets/X_test.csv')\n",
    "X_valid = pd.read_csv('../cleaned_datasets/X_valid.csv')\n",
    "\n",
    "y_train = pd.read_csv('../cleaned_datasets/y_train.csv')\n",
    "y_test = pd.read_csv('../cleaned_datasets/y_test.csv')\n",
    "y_valid = pd.read_csv('../cleaned_datasets/y_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b204bb",
   "metadata": {},
   "source": [
    "## Baseline Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42aec741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Baseline Model Accuracy:  0.8651994497936726\n",
      "XGBoost Baseline Confusion Matrix: \n",
      "[[599  73]\n",
      " [ 25  30]]\n",
      "\n",
      "XGBoost Baseline Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92       672\n",
      "           1       0.29      0.55      0.38        55\n",
      "\n",
      "    accuracy                           0.87       727\n",
      "   macro avg       0.63      0.72      0.65       727\n",
      "weighted avg       0.91      0.87      0.88       727\n",
      "\n",
      "\n",
      "XGBoost Baseline Precision Score: 0.2912621359223301\n",
      "\n",
      "XGBoost Baseline Recall Score: 0.5454545454545454\n",
      "\n",
      "XGBoost Baseline F1 Score: 0.37974683544303794\n",
      "\n",
      "XGBoost Baseline ROC AUC Score: 0.7184117965367965\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "xgb_baseline_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_baseline_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred_xgb_baseline = xgb_baseline_model.predict(X_valid)\n",
    "\n",
    "print(\"XGBoost Baseline Model Accuracy: \", accuracy_score(y_valid, y_pred_xgb_baseline))\n",
    "print(\"XGBoost Baseline Confusion Matrix: \")\n",
    "print(confusion_matrix(y_valid, y_pred_xgb_baseline))\n",
    "print(\"\\nXGBoost Baseline Classification Report:\\n\", classification_report(y_valid, y_pred_xgb_baseline))\n",
    "print(\"\\nXGBoost Baseline Precision Score:\", precision_score(y_valid, y_pred_xgb_baseline))\n",
    "print(\"\\nXGBoost Baseline Recall Score:\", recall_score(y_valid, y_pred_xgb_baseline))\n",
    "print(\"\\nXGBoost Baseline F1 Score:\", f1_score(y_valid, y_pred_xgb_baseline))\n",
    "print(\"\\nXGBoost Baseline ROC AUC Score:\", roc_auc_score(y_valid, y_pred_xgb_baseline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6df50",
   "metadata": {},
   "source": [
    "Here is an interpretation of the XGBoost baseline model performance:\n",
    "\n",
    "**Accuracy**: The accuracy score is 86.52%, which means that the model is correctly classifying the instances around 86.5% of the time. However, as mentioned before, accuracy can be misleading when dealing with imbalanced datasets, so it is essential to look at other metrics as well.\n",
    "\n",
    "**Confusion Matrix**: The confusion matrix provides a detailed view of the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). In this case:\n",
    "   1. 599 instances were correctly classified as non-hazardous (TN).\n",
    "   2. 30 instances were correctly classified as hazardous (TP).\n",
    "   3. 73 instances were classified as hazardous but were actually non-hazardous (FP).\n",
    "   4. 25 instances were classified as non-hazardous but were actually hazardous (FN).\n",
    "\n",
    "**Classification Report**:\n",
    "\n",
    "**Precision**: Precision is the ratio of true positives (TP) to the sum of true positives and false positives (FP). It measures the proportion of correct positive predictions out of all positive predictions made by the model.\n",
    "   1. For class 0 (non-hazardous): The model has a precision of 96%, meaning that 96% of the instances predicted as non-hazardous were indeed non-hazardous.\n",
    "   2. For class 1 (hazardous): The model has a precision of 29.13%, meaning that around 29.13% of the instances predicted as hazardous were actually hazardous.\n",
    "\n",
    "**Recall**: Recall is the ratio of true positives (TP) to the sum of true positives and false negatives (FN). It measures the proportion of actual positive instances that were correctly identified by the model.\n",
    "   1. For class 0 (non-hazardous): The model has a recall of 89%, meaning that it correctly identified 89% of the non-hazardous instances.\n",
    "   2. For class 1 (hazardous): The model has a recall of 54.55%, meaning that it correctly identified 54.55% of the hazardous instances.\n",
    "\n",
    "**F1-score**: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure of a model's performance, especially when dealing with imbalanced datasets.\n",
    "   1. For class 0 (non-hazardous): The model has an F1-score of 92%, which indicates a good balance between precision and recall for the non-hazardous class.\n",
    "   2. For class 1 (hazardous): The model has an F1-score of 37.97%, which indicates a relatively poor balance between precision and recall for the hazardous class.\n",
    "\n",
    "**Support**: Support is the number of actual occurrences of each class in the dataset.\n",
    "   1. For class 0 (non-hazardous): There are 672 instances in the dataset.\n",
    "   2. For class 1 (hazardous): There are 55 instances in the dataset.\n",
    "\n",
    "**Precision Score**: The precision score for the positive (hazardous) class is 29.13%, which indicates that out of all the instances predicted as hazardous, only 29.13% were actually hazardous.\n",
    "\n",
    "**Recall Score**: The recall score for the positive (hazardous) class is 54.55%, which indicates that out of all the actual hazardous instances, the model correctly identified 54.55% of them.\n",
    "\n",
    "**F1 Score**: The F1 score for the positive (hazardous) class is 37.97%, which is the harmonic mean of precision and recall. It gives a balanced measure of the model's performance on the positive class, especially when there is an imbalance in the dataset.\n",
    "\n",
    "**ROC AUC Score**: The ROC AUC score is 71.84%, which measures the model's ability to distinguish between the two classes. A score of 100% would indicate a perfect classifier, while a score of 50% means the model is no better than random guessing. A score of 71.84% suggests that the model has moderate discrimination power.\n",
    "\n",
    "The XGBoost baseline model performs better than the Random Forest baseline model in terms of accuracy, precision, F1 score, and ROC AUC score. However, it still struggles with the hazardous class. This is likely due to the imbalanced nature of the validation dataset, and further optimization should help improve the model's performance on the hazardous class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf3f8a",
   "metadata": {},
   "source": [
    "## Hyperparamter Tuning using GridSearchCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeceb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_child_weight': [1, 2, 4],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.5, 0.8, 1],\n",
    "    'colsample_bytree': [0.5, 0.8, 1]\n",
    "}\n",
    "\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb.XGBClassifier(random_state=42), param_grid=param_grid, cv=5, verbose=0, n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train, y_train.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search_xgb.best_params_\n",
    "print(\"Best Parameters: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61671929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned XGBoost Model Accuracy:  0.8638239339752407\n",
      "Tuned XGBoost Confusion Matrix: \n",
      "[[600  72]\n",
      " [ 27  28]]\n",
      "\n",
      "Tuned XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92       672\n",
      "           1       0.28      0.51      0.36        55\n",
      "\n",
      "    accuracy                           0.86       727\n",
      "   macro avg       0.62      0.70      0.64       727\n",
      "weighted avg       0.91      0.86      0.88       727\n",
      "\n",
      "\n",
      "Tuned XGBoost Precision Score: 0.28\n",
      "\n",
      "Tuned XGBoost Recall Score: 0.509090909090909\n",
      "\n",
      "Tuned XGBoost F1 Score: 0.36129032258064514\n",
      "\n",
      "Tuned XGBoost ROC AUC Score: 0.7009740259740259\n"
     ]
    }
   ],
   "source": [
    "tuned_xgb_model = xgb.XGBClassifier(\n",
    "    colsample_bytree=0.5,\n",
    "    gamma=0.2,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=20,\n",
    "    min_child_weight=1,\n",
    "    n_estimators=200,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tuned_xgb_model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred_tuned_xgb = tuned_xgb_model.predict(X_valid)\n",
    "\n",
    "print(\"Tuned XGBoost Model Accuracy: \", accuracy_score(y_valid, y_pred_tuned_xgb))\n",
    "print(\"Tuned XGBoost Confusion Matrix: \")\n",
    "print(confusion_matrix(y_valid, y_pred_tuned_xgb))\n",
    "print(\"\\nTuned XGBoost Classification Report:\\n\", classification_report(y_valid, y_pred_tuned_xgb))\n",
    "print(\"\\nTuned XGBoost Precision Score:\", precision_score(y_valid, y_pred_tuned_xgb))\n",
    "print(\"\\nTuned XGBoost Recall Score:\", recall_score(y_valid, y_pred_tuned_xgb))\n",
    "print(\"\\nTuned XGBoost F1 Score:\", f1_score(y_valid, y_pred_tuned_xgb))\n",
    "print(\"\\nTuned XGBoost ROC AUC Score:\", roc_auc_score(y_valid, y_pred_tuned_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feeb7d2",
   "metadata": {},
   "source": [
    "Here is the interpretation of the performance metrics for the tuned XGBoost model on the validation dataset:\n",
    "\n",
    "**Accuracy**: The accuracy score is 86.38%, which means that the model is correctly classifying the instances around 86% of the time. However, accuracy can be misleading when dealing with imbalanced datasets, so it is essential to look at other metrics as well.\n",
    "\n",
    "**Confusion Matrix**: The confusion matrix provides a detailed view of the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). In this case:\n",
    "   1. 600 instances were correctly classified as non-hazardous (TN).\n",
    "   2. 28 instances were correctly classified as hazardous (TP).\n",
    "   3. 72 instances were classified as hazardous but were actually non-hazardous (FP).\n",
    "   4. 27 instances were classified as non-hazardous but were actually hazardous (FN).\n",
    "\n",
    "**Classification Report**:\n",
    "\n",
    "**Precision**: Precision is the ratio of true positives (TP) to the sum of true positives and false positives (FP). It measures the proportion of correct positive predictions out of all positive predictions made by the model.\n",
    "   1. For class 0 (non-hazardous): The model has a precision of 96%, meaning that 96% of the instances predicted as non-hazardous were indeed non-hazardous.\n",
    "   2. For class 1 (hazardous): The model has a precision of 28%, meaning that only 28% of the instances predicted as hazardous were actually hazardous.\n",
    "\n",
    "**Recall**: Recall is the ratio of true positives (TP) to the sum of true positives and false negatives (FN). It measures the proportion of actual positive instances that were correctly identified by the model.\n",
    "   1. For class 0 (non-hazardous): The model has a recall of 89%, meaning that it correctly identified 89% of the non-hazardous instances.\n",
    "   2. For class 1 (hazardous): The model has a recall of 50.91%, meaning that it correctly identified around 51% of the hazardous instances.\n",
    "\n",
    "**F1-score**: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure of a model's performance, especially when dealing with imbalanced datasets.\n",
    "   1. For class 0 (non-hazardous): The model has an F1-score of 92%, which indicates a good balance between precision and recall for the non-hazardous class.\n",
    "   2. For class 1 (hazardous): The model has an F1-score of 36.13%, which indicates a relatively poor balance between precision and recall for the hazardous class.\n",
    "\n",
    "**Precision**: The precision score for the positive (hazardous) class is 28%, which indicates that out of all the instances predicted as hazardous, only 28% were actually hazardous.\n",
    "\n",
    "**Recall**: The recall score for the positive (hazardous) class is 50.91%, which indicates that out of all the actual hazardous instances, the model correctly identified around 51% of them.\n",
    "\n",
    "**F1 Score**: The F1 score for the positive (hazardous) class is 36.13%, which is the harmonic mean of precision and recall. It gives a balanced measure of the model's performance on the positive class, especially when there is an imbalance in the dataset.\n",
    "\n",
    "**ROC AUC Score**: The ROC AUC score is 70.10%, which measures the model's ability to distinguish between the two classes. A score of 100% would indicate a perfect classifier, while a score of 50% means the model is no better than random guessing. A score of 70% suggests that the model has moderate discrimination power.\n",
    "\n",
    "The tuned XGBoost model performs well on the non-hazardous class but still struggles with the hazardous class. This is likely due to the imbalanced nature of the validation dataset. Although the tuned model shows some improvement compared to the baseline model, further optimization and experimentation with different techniques like oversampling or undersampling might help improve the model's performance on the hazardous class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
